{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073bf8f9",
   "metadata": {},
   "source": [
    "# Models, Prompts and Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80482d1",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a525b58",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install langchain langchain-community langchain-ollama langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c5b27",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Use Ollama to get a local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d4a269",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.2:1b', temperature=0.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat = ChatOllama(model='llama3.2:1b', temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07256",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "Prompts helps on standardizing or adding more context to the LLM beyond user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a31f246",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template: input_variables=['style', 'text'] input_types={} partial_variables={} template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'\n",
      "Prompt input variables: ['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "print(f\"Prompt template: {prompt_template.messages[0].prompt}\")\n",
    "print(f\"Prompt input variables: {prompt_template.messages[0].prompt.input_variables}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf258b5",
   "metadata": {},
   "source": [
    "Create customer message prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd51a93",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result type: <class 'list'>\n",
      "Message Type: <class 'langchain_core.messages.human.HumanMessage'>\n",
      "Message: content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_messages = prompt_template.format_messages(style=customer_style, text=customer_email)\n",
    "\n",
    "print(f\"Result type: {type(customer_messages)}\")\n",
    "print(f\"Message Type: {type(customer_messages[0])}\")\n",
    "print(f\"Message: {customer_messages[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd7cea",
   "metadata": {},
   "source": [
    "Call the LLM to translate to the style of the customer message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd789f9f",
   "metadata": {
    "height": 62,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Here's a rewritten version of the text in an American English style, in a calm and respectful tone:\n",
      "\n",
      "\"I'm really upset about my blender lid flying off and leaving a mess on my kitchen walls with smoothies. To make things worse, I don't have the insurance coverage to cover the cost of cleaning up this mess. That's why I was wondering if you could help me out right away.\"\n"
     ]
    }
   ],
   "source": [
    "customer_response = chat.invoke(customer_messages)\n",
    "print(f\"Response: {customer_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7acd56",
   "metadata": {},
   "source": [
    "Create prompt template for service reply (reuse customer template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c267e5f",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Arrr, me hearty customer be wantin' a translation o' that scurvy text, eh? Alright then, let's set sail fer a polite tone, savvy?\n",
      "\n",
      "```text\n",
      "\"Hey there, valued customer, we regret to inform ye that our warranty doesn't cover the costs associated with cleanin' yer kitchen. It seems ye've been havin' a bit o' trouble keepin' yer blender in one piece, and for that, we can only offer our sincerest apologies. We hope ye'll take this as a reminder to always follow proper safety protocols when usin' yer appliances. Fair winds and following seas!\"\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\"\n",
    "\n",
    "service_messages = prompt_template.format_messages(style=service_style_pirate, text=service_reply)\n",
    "service_response = chat.invoke(service_messages)\n",
    "\n",
    "print(f\"Response: {service_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36536e79",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:\n",
    "\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}\n",
    "\n",
    "Even by using the chat prompt template and specify about returning or extract a json file, the response will be a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0f4680",
   "metadata": {
    "height": 555,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'str'>\n",
      "Response: Here is the extracted information in the required format:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"It arrived in two days\", \"I've been using it every other morning to clear the leaves on our lawn\"]\n",
      "}\n",
      "```\n",
      "\n",
      "Note: The price value sentences are not directly related to the gift or delivery information, but rather a separate observation about the product. If you'd like to include them in the output as well, I can modify the code accordingly.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing. It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "print(f\"Response Type: {type(response.content)}\")\n",
    "print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7de2b8",
   "metadata": {},
   "source": [
    "For this reason, it is necessary to use output parsers.\n",
    "\n",
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e0ec49",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased    as a gift for someone else?     Answer True if yes,    False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days    did it take for the product    to arrive? If this     information is not found,    output -1.\n",
      "\t\"price_value\": string  // Extract any    sentences about the value or     price, and output them as a     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(\n",
    "  name=\"gift\",\n",
    "  description=\"Was the item purchased\\\n",
    "    as a gift for someone else? \\\n",
    "    Answer True if yes,\\\n",
    "    False if not or unknown.\"\n",
    ")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(\n",
    "  name=\"delivery_days\",\n",
    "  description=\"How many days\\\n",
    "    did it take for the product\\\n",
    "    to arrive? If this \\\n",
    "    information is not found,\\\n",
    "    output -1.\"\n",
    ")\n",
    "\n",
    "price_value_schema = ResponseSchema(\n",
    "  name=\"price_value\",\n",
    "  description=\"Extract any\\\n",
    "    sentences about the value or \\\n",
    "    price, and output them as a \\\n",
    "    comma separated Python list.\"\n",
    ")\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas([gift_schema, delivery_days_schema, price_value_schema])\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "082947fc",
   "metadata": {
    "height": 385,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing. It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased    as a gift for someone else?     Answer True if yes,    False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days    did it take for the product    to arrive? If this     information is not found,    output -1.\n",
      "\t\"price_value\": string  // Extract any    sentences about the value or     price, and output them as a     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_template_with_instructions = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_with_instructions)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)\n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b663657",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": \"2\",\n",
      "    \"price_value\": \"slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904f1c25",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Dict Type: <class 'dict'>\n",
      "Output Dict: {'gift': True, 'delivery_days': '2', 'price_value': \"slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}\n",
      "Delivery Days: 2\n"
     ]
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "print(f\"Output Dict Type: {type(output_dict)}\")\n",
    "print(f\"Output Dict: {output_dict}\")\n",
    "print(f\"Delivery Days: {output_dict['delivery_days']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningai-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
